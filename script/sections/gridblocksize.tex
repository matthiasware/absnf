\section{Gridsize and Blocksize} \label{sec_blockgridsize}
All of our self written kernels have to operate on matrices and vectors. Usually the single entries of the structures do not depend on the others and can be independently and therefore in parallel calculated.
\begin{center}
	EXAMPLE + MORE EXAMPLE LISTS
\end{center}
The questions while implementing this operation were:
\begin{enumerate}
	\item How to step through the data-structures in order to minimize cache-misses?
	\item How to choose the grid-size?
	\item How to choose the block-size?
\end{enumerate}

Obviously the most efficient and preferment answer to this question is to decide problem specific and device specific. Unfortunately this is also the most time consuming approach.\\
In our case we tried to find a generic solution, that performs well enough to not deteriorate the overall performance. \\

The basic idea is:
\begin{itemize}
	\item Chose and fix blocksize and gridsize depending on the device properties
	\item Start kernel with given blocksize and gridsize
	\item Each thread can be responsible for multiple tasks and chooses its next task after the current one is done or terminates
\end{itemize}

The basic algorithm  for kernels of this type is listed in \ref{lst_bg_size}

\begin{lstlisting}[caption={\label{lst_bg_size}}, language=cpp]
template <typename T>
void _global_ row_wise_traversal(T *matrix, int s)
{
	int i = blockIdx.x;
	int j = threadIdx.x;
	int global_id = blockIdx.x * blockDim.x + threadIdx.x;
	int id = i*s + j;
	int size = s*s;
	while(id < size && i < s)
	{
		matrix[id] = doSomething();
		j += blockDim.x;
		if (j>=s)
		{
			j = j % s;
			i = i + gridDim.x;
		}
		id = i*s+j;
	}
}
\end{lstlisting}
Here each block is assigned a row of the matrix. Threads operate on these rows and calculate autonomously their next task.

\subsection{Choosing the right blocksize and gridsize}

Ideally we want to achieve the following objectives:
\begin{enumerate}
	\item Utilize all the MPUs to capacity
	\item Minimizing cache misses
\end{enumerate}

We can minimize cache misses by assigning threads of the same block and and the same warp adjacent matrix entries. This is automatically achieved by assigning matrix rows to blocks.
To ensure that each Core of the MPU runs on maximal capacity, we start at least as many threads as a MPU can execute in parallel. This obviously finds its limit with the constant max\_threads\_per\_block.

\begin{center}
	TABLE WHITEPAPERg
\end{center}

\subsection{Performance}

\begin{center}
	GRAPHICS
\end{center}

\begin{center}
	GRAPHICS 2
\end{center}

\subsection{Notes}
Our approach obviously lacks. Device specfications are not taken in consideration. SP, DP.